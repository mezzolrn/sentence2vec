{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocessor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = range(10)\n",
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 'asdkjdjdsdlk'\n",
    "t = len(b)-2\n",
    "i = 0\n",
    "while i < t:\n",
    "    if b[i] == b[i+2]:\n",
    "        b = b[:i] + b[i+3:]\n",
    "        t -= 3\n",
    "    else:\n",
    "        i += 1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! LANG=C.UTF-8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source /etc/profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ENV LANG C.UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!locale -a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(classification_report([1],[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- encoding:utf8 -*-\n",
    "from utils import Dictionary\n",
    "from model import Classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "P2P = ''\n",
    "# ****************************** 参数设置 ******************************************* #\n",
    "DICT_PATH = './preprocessed_data/'+P2P+'mydict(all).json'\n",
    "DATA_PATH = './preprocessed_data/'+P2P+'data(all).json'\n",
    "MODEL_PATH = './models/'+P2P+'bestmodel-%f-%f.pt'\n",
    "LOG_PATH = './logs/'+P2P+'log.txt'\n",
    "BATCH_SIZE = 5\n",
    "DROPOUT = 0.3\n",
    "N_LAYERS = 2\n",
    "HIDDEN_DIM = 300\n",
    "EMBED_DIM = 50\n",
    "POOLING = 'all'\n",
    "ATTENTION_UNIT = 350\n",
    "ATTENTION_HOPS = 4\n",
    "MAX_LEN = 500\n",
    "PRETRAINED_WORDVEC = False\n",
    "N_HIDDEN_UNITS = 300\n",
    "N_CLASSES = 3\n",
    "LR = 0.001\n",
    "EPOCH = 10\n",
    "USE_ATTENTION = True\n",
    "PENALIZATION_COEFF = 1.0\n",
    "CLIP = 0.5\n",
    "N_TRAIN = 0.8\n",
    "N_DEV = 0.1\n",
    "N_VAL = 0.1\n",
    "EARLY_STOP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算矩阵的F范数\n",
    "def Frobenius(mat):\n",
    "    size = mat.size()\n",
    "    if len(size) == 3:  # batched matrix\n",
    "        ret = (torch.sum(torch.sum((mat ** 2), 2), 1).squeeze() + 1e-10) ** 0.5\n",
    "        return torch.sum(ret) / size[0]\n",
    "    else:\n",
    "        raise Exception('matrix for computing Frobenius norm should be with 3 dims')\n",
    "\n",
    "\n",
    "def evaluate(model, loss_func, dictionary, data):\n",
    "    \"\"\"evaluate the model while training\"\"\"\n",
    "    model.eval()  # turn on the eval() switch to disable dropout\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    ####3\n",
    "    total_prediction = []\n",
    "    total_labels = []\n",
    "    \n",
    "    for texts, labels, masks, bsz in utils.getBatch(data=data, dictionary=dictionary, maxlen=MAX_LEN,\n",
    "                                                    batch_size=BATCH_SIZE):\n",
    "        hidden = model.init_hidden(texts.size(0))\n",
    "        output, attention = model.forward(texts, masks, hidden)\n",
    "        output_flat = output.view(texts.size(0), -1)\n",
    "        total_loss += loss_func(output_flat, labels).data\n",
    "        prediction = torch.max(output_flat, 1)[1]\n",
    "        total_correct += torch.sum((prediction == labels).float())\n",
    "        \n",
    "        total_prediction += prediction\n",
    "        total_labels += labels\n",
    "    print(classification_report(total_labels,total_prediction))\n",
    "    return total_loss[0] / (len(data) // BATCH_SIZE), total_correct.data[0] / len(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
